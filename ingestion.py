import os
import sys
import warnings
import pandas as pd
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain_core.documents import Document

# UyarÄ±larÄ± bastÄ±r
warnings.filterwarnings("ignore")

load_dotenv()

# --- AYARLAR ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
DATA_FOLDER = "./data"
MAX_FILE_SIZE_MB = 50  # 50 MB'dan bÃ¼yÃ¼k dosyalarÄ± atla

# Ã–ÄŸretmek istediÄŸin web sitelerinin linklerini buraya ekle:
URL_LIST = [
    "https://www.aid.org.tr/hastaliklar/alerji-ve-bagisiklik-sistemi-hastaliklari/gida-alerjisi/",
    "https://istanbulalerjimerkezi.com.tr/alerji-nedir-belirtileri-nelerdir/",
    "https://www.aid.org.tr/",
    "https://alerjiastim.org.tr/"
]

if not GOOGLE_API_KEY:
    print("âŒ HATA: .env dosyasÄ±nda API Key bulunamadÄ±!")
    sys.exit(1)

# --- Ä°STATÄ°STÄ°KLER ---
stats = {
    "pdf_success": 0,
    "pdf_failed": 0,
    "pdf_skipped": 0,
    "csv_success": 0,
    "web_success": 0,
    "total_pages": 0
}
failed_files = []
skipped_files = []

# --- YARDIMCI FONKSÄ°YONLAR ---
def get_file_size_mb(file_path):
    """Dosya boyutunu MB olarak dÃ¶ndÃ¼rÃ¼r."""
    return os.path.getsize(file_path) / (1024 * 1024)

def load_single_pdf(file_path):
    """Tek bir PDF dosyasÄ±nÄ± gÃ¼venli ÅŸekilde yÃ¼kler."""
    filename = os.path.basename(file_path)
    
    # Dosya boyutu kontrolÃ¼
    size_mb = get_file_size_mb(file_path)
    if size_mb > MAX_FILE_SIZE_MB:
        print(f"  â­ï¸  {filename} ({size_mb:.1f} MB) - Ã‡ok bÃ¼yÃ¼k, atlanÄ±yor...")
        skipped_files.append(f"{filename} ({size_mb:.1f} MB)")
        stats["pdf_skipped"] += 1
        return []
    
    try:
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        page_count = len(docs)
        stats["pdf_success"] += 1
        stats["total_pages"] += page_count
        print(f"  âœ… {filename} ({page_count} sayfa)")
        return docs
    except Exception as e:
        error_msg = str(e)[:50]  # HatayÄ± kÄ±salt
        print(f"  âŒ {filename} - Hata: {error_msg}")
        failed_files.append(filename)
        stats["pdf_failed"] += 1
        return []

def load_structured_data(file_path):
    """CSV veya Excel dosyasÄ±nÄ± yÃ¼kleyip Document listesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r."""
    filename = os.path.basename(file_path)
    try:
        if file_path.endswith('.csv'):
            # FarklÄ± encoding'leri dene
            for encoding in ['utf-8', 'latin-1', 'cp1254', 'iso-8859-9']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            else:
                df = pd.read_csv(file_path, encoding='utf-8', errors='ignore')
        else:
            df = pd.read_excel(file_path)
        
        documents = []
        for _, row in df.iterrows():
            content = " | ".join([f"{col}: {val}" for col, val in row.items()])
            documents.append(Document(page_content=content, metadata={"source": file_path}))
        
        stats["csv_success"] += 1
        print(f"  âœ… {filename} ({len(df)} satÄ±r)")
        return documents
    except Exception as e:
        print(f"  âŒ {filename} - Hata: {e}")
        failed_files.append(filename)
        return []

# --- BANNER ---
print("\n" + "=" * 60)
print("ğŸ§¬ ALERJÄ° CHATBOT - VERÄ° YÃœKLEME SÄ°STEMÄ°")
print("=" * 60 + "\n")

all_documents = []

# --- 1. PDF DOSYALARINI YÃœKLE ---
print("ğŸ“‚ PDF DosyalarÄ± YÃ¼kleniyor...")
print("-" * 40)

if os.path.exists(DATA_FOLDER):
    pdf_files = [f for f in os.listdir(DATA_FOLDER) if f.lower().endswith('.pdf')]
    
    if pdf_files:
        for pdf_file in sorted(pdf_files):
            file_path = os.path.join(DATA_FOLDER, pdf_file)
            docs = load_single_pdf(file_path)
            all_documents.extend(docs)
    else:
        print("  â„¹ï¸ PDF dosyasÄ± bulunamadÄ±.")
else:
    print("  âš ï¸ 'data' klasÃ¶rÃ¼ bulunamadÄ±!")

print()

# --- 2. CSV / EXCEL DOSYALARINI YÃœKLE ---
print("ğŸ“Š CSV/Excel DosyalarÄ± YÃ¼kleniyor...")
print("-" * 40)

if os.path.exists(DATA_FOLDER):
    data_files = [f for f in os.listdir(DATA_FOLDER) 
                  if f.lower().endswith(('.csv', '.xlsx', '.xls'))]
    
    if data_files:
        for data_file in sorted(data_files):
            file_path = os.path.join(DATA_FOLDER, data_file)
            docs = load_structured_data(file_path)
            all_documents.extend(docs)
    else:
        print("  â„¹ï¸ CSV/Excel dosyasÄ± bulunamadÄ±.")
else:
    print("  âš ï¸ 'data' klasÃ¶rÃ¼ bulunamadÄ±!")

print()

# --- 3. WEB SÄ°TELERÄ°NÄ° YÃœKLE ---
print("ğŸŒ Web Siteleri TaranÄ±yor...")
print("-" * 40)

if URL_LIST:
    for url in URL_LIST:
        try:
            loader = WebBaseLoader([url])
            docs = loader.load()
            all_documents.extend(docs)
            stats["web_success"] += 1
            # URL'yi kÄ±salt
            short_url = url.replace("https://", "").replace("http://", "")[:40]
            print(f"  âœ… {short_url}...")
        except Exception as e:
            print(f"  âŒ {url[:40]}... - Hata")
else:
    print("  â„¹ï¸ URL listesi boÅŸ.")

print()

# --- SONUÃ‡ KONTROLÃœ ---
if not all_documents:
    print("=" * 60)
    print("âŒ HÄ°Ã‡BÄ°R VERÄ° KAYNAÄI YÃœKLENEMEDÄ°!")
    print("=" * 60)
    sys.exit(1)

# --- 4. PARÃ‡ALAMA (CHUNKING) ---
print("âœ‚ï¸  Metinler ParÃ§alanÄ±yor...")
print("-" * 40)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, 
    chunk_overlap=100,
    separators=["\n\n", "\n", ". ", " ", ""]
)
splits = text_splitter.split_documents(all_documents)
print(f"  ğŸ“„ {len(all_documents)} dÃ¶kÃ¼man â†’ {len(splits)} parÃ§a")
print()

# --- 5. VEKTÃ–R VERÄ°TABANI (CHROMA) ---
print("ğŸ’¾ VeritabanÄ±na YazÄ±lÄ±yor...")
print("-" * 40)
print(f"  â³ {len(splits)} parÃ§a iÅŸleniyor... (Bu biraz sÃ¼rebilir)")

embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

# Mevcut veritabanÄ±nÄ± temizle ve yeniden oluÅŸtur
import shutil
if os.path.exists("./chroma_db"):
    shutil.rmtree("./chroma_db")

db = Chroma.from_documents(
    documents=splits, 
    embedding=embeddings, 
    persist_directory="./chroma_db"
)

print("  âœ… VeritabanÄ± baÅŸarÄ±yla oluÅŸturuldu!")
print()

# --- Ã–ZET RAPOR ---
print("=" * 60)
print("ğŸ“Š YÃœKLEME RAPORU")
print("=" * 60)
print(f"""
  ğŸ“„ PDF DosyalarÄ±:
     â€¢ BaÅŸarÄ±lÄ±: {stats['pdf_success']} dosya ({stats['total_pages']} sayfa)
     â€¢ BaÅŸarÄ±sÄ±z: {stats['pdf_failed']} dosya
     â€¢ Atlanan (bÃ¼yÃ¼k): {stats['pdf_skipped']} dosya

  ğŸ“Š CSV/Excel: {stats['csv_success']} dosya
  ğŸŒ Web Siteleri: {stats['web_success']} site
  
  ğŸ“¦ Toplam: {len(splits)} metin parÃ§asÄ± veritabanÄ±na yazÄ±ldÄ±
""")

if failed_files:
    print("  âš ï¸ Okunamayan Dosyalar:")
    for f in failed_files:
        print(f"     â€¢ {f}")
    print()

if skipped_files:
    print("  â­ï¸ Atlanan Dosyalar (Ã‡ok BÃ¼yÃ¼k):")
    for f in skipped_files:
        print(f"     â€¢ {f}")
    print()

print("=" * 60)
print("âœ… Ä°ÅLEM TAMAMLANDI! Bot eÄŸitildi ve hazÄ±r.")
print("=" * 60 + "\n")
