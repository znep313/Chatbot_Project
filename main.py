import os
import sys
from dotenv import load_dotenv

# Sadece temel parÃ§alarÄ± Ã§aÄŸÄ±rÄ±yoruz, karmaÅŸÄ±k zincirleri deÄŸil.
try:
    from langchain_chroma import Chroma
    from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
except ImportError as e:
    print(f"âŒ KÃ¼tÃ¼phane eksik: {e}")
    sys.exit(1)

# 1. Ayarlar
load_dotenv()

if "GOOGLE_API_KEY" not in os.environ:
    print("âŒ HATA: .env dosyasÄ±nda GOOGLE_API_KEY bulunamadÄ±.")
    sys.exit(1)

print("ğŸ¤– Alerji AsistanÄ± (Manuel Mod) BaÅŸlatÄ±lÄ±yor...")

# 2. Modelleri HazÄ±rla
# Not: Ingestion.py ile aynÄ± embedding modelini kullanmak zorundayÄ±z.
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

# Beyin (LLM)
llm = ChatGoogleGenerativeAI(
    model="gemini-3-flash-preview",
    temperature=0.3
)

# 3. VeritabanÄ±na BaÄŸlan
if not os.path.exists("./chroma_db"):
    print("âŒ HATA: 'chroma_db' klasÃ¶rÃ¼ yok. Ã–nce veriyi yÃ¼klemek iÃ§in ingestion.py Ã§alÄ±ÅŸtÄ±r.")
    sys.exit(1)

db = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)

print("\nâœ… SÄ°STEM HAZIR! (Ã‡Ä±kmak iÃ§in 'q')\n")

# --- SOHBET DÃ–NGÃœSÃœ ---
while True:
    user_input = input("Siz: ")
    
    if user_input.lower() in ['q', 'exit', 'cikis']:
        print("GÃ¶rÃ¼ÅŸmek Ã¼zere!")
        break
    
    if not user_input.strip():
        continue

    print("ğŸ” Bilgi aranÄ±yor ve cevaplanÄ±yor...")

    try:
        # ADIM A: RETRIEVAL (Bilgi Getirme)
        # Zincir yerine veritabanÄ±na doÄŸrudan "Bana buna benzer 3 parÃ§a getir" diyoruz.
        relevant_docs = db.similarity_search(user_input, k=3)
        
        # Bulunan metinleri tek bir paragraf haline getiriyoruz (Context oluÅŸturma)
        context_text = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        if not context_text:
            context_text = "VeritabanÄ±nda ilgili bilgi bulunamadÄ±."

        # ADIM B: AUGMENTATION (Ä°stemi HazÄ±rlama)
        # LLM'e gÃ¶ndereceÄŸimiz mesajÄ± f-string ile elle yazÄ±yoruz.
        final_prompt = f"""
        Sen uzman bir alerji asistanÄ±sÄ±n. AÅŸaÄŸÄ±daki "BULUNAN BÄ°LGÄ°" kÄ±smÄ±nÄ± kullanarak soruyu cevapla.
        EÄŸer bilgi metinde yoksa, kendi kafandan uydurma, "Bu konuda bilgim yok" de.

        SORU: {user_input}

        BULUNAN BÄ°LGÄ° (CONTEXT):
        {context_text}

        CEVAP:
        """

        # ADIM C: GENERATION (Cevap Ãœretme)
        # HazÄ±rladÄ±ÄŸÄ±mÄ±z metni direkt modele veriyoruz.
        response = llm.invoke(final_prompt)
        
        print(f"Bot: {response.content}\n")
        
    except Exception as e:
        print(f"âŒ Bir hata oldu: {e}")